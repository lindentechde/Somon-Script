import { Lexer } from '../src/lexer';
import { TokenType } from '../src/types';

function tokenize(source: string) {
  const lexer = new Lexer(source);
  return lexer.tokenize();
}

describe('Lexer Extended Coverage Tests', () => {
  describe('BOM handling', () => {
    test('should handle BOM at start of file', () => {
      const sourceWithBOM = '\uFEFFÑ‚Ð°Ò“Ð¹Ð¸Ñ€Ñ‘Ð±Ð°Ð½Ð´Ð° Ð½Ð¾Ð¼ = "Ñ‚ÐµÑÑ‚";';
      const tokens = tokenize(sourceWithBOM);

      expect(tokens[0].type).toBe(TokenType.Ð¢ÐÒ’Ð™Ð˜Ð ÐÐ‘ÐÐÐ”Ð);
      expect(tokens[1].value).toBe('Ð½Ð¾Ð¼');
    });

    test('should handle file without BOM', () => {
      const sourceWithoutBOM = 'Ñ‚Ð°Ò“Ð¹Ð¸Ñ€Ñ‘Ð±Ð°Ð½Ð´Ð° Ð½Ð¾Ð¼ = "Ñ‚ÐµÑÑ‚";';
      const tokens = tokenize(sourceWithoutBOM);

      expect(tokens[0].type).toBe(TokenType.Ð¢ÐÒ’Ð™Ð˜Ð ÐÐ‘ÐÐÐ”Ð);
      expect(tokens[1].value).toBe('Ð½Ð¾Ð¼');
    });
  });

  describe('Comment handling edge cases', () => {
    test('should handle line comment at end of file without newline', () => {
      const source = 'Ñ‚Ð°Ò“Ð¹Ð¸Ñ€Ñ‘Ð±Ð°Ð½Ð´Ð° Ñ… = 5; // ÐºÐ¾Ð¼Ð¼ÐµÐ½Ñ‚';
      const tokens = tokenize(source);

      expect(tokens.length).toBeGreaterThanOrEqual(6); // Ñ‚Ð°Ò“Ð¹Ð¸Ñ€Ñ‘Ð±Ð°Ð½Ð´Ð°, Ñ…, =, 5, ;, EOF (and maybe extras)
      expect(tokens[tokens.length - 1].type).toBe(TokenType.EOF);
    });

    test('should handle multiple line comments', () => {
      const source = `// ÐºÐ¾Ð¼Ð¼ÐµÐ½Ñ‚ 1
Ñ‚Ð°Ò“Ð¹Ð¸Ñ€Ñ‘Ð±Ð°Ð½Ð´Ð° Ñ… = 5; // ÐºÐ¾Ð¼Ð¼ÐµÐ½Ñ‚ 2
// ÐºÐ¾Ð¼Ð¼ÐµÐ½Ñ‚ 3`;
      const tokens = tokenize(source);

      // Should only have the variable declaration tokens
      const nonEofTokens = tokens.filter(
        t => t.type !== TokenType.EOF && t.type !== TokenType.NEWLINE
      );
      expect(nonEofTokens).toHaveLength(5); // Ñ‚Ð°Ò“Ð¹Ð¸Ñ€Ñ‘Ð±Ð°Ð½Ð´Ð°, Ñ…, =, 5, ;
    });

    test('should handle empty line comment', () => {
      const source = 'Ñ‚Ð°Ò“Ð¹Ð¸Ñ€Ñ‘Ð±Ð°Ð½Ð´Ð° Ñ… = 5; //\nÑ„ÑƒÐ½ÐºÑÐ¸Ñ Ñ‚ÐµÑÑ‚() {}';
      const tokens = tokenize(source);

      const keywordTokens = tokens.filter(
        t => t.type === TokenType.Ð¢ÐÒ’Ð™Ð˜Ð ÐÐ‘ÐÐÐ”Ð || t.type === TokenType.Ð¤Ð£ÐÐšÐ¡Ð˜Ð¯
      );
      expect(keywordTokens).toHaveLength(2);
    });
  });

  describe('String literal edge cases', () => {
    test('should handle empty string', () => {
      const source = 'Ñ‚Ð°Ò“Ð¹Ð¸Ñ€Ñ‘Ð±Ð°Ð½Ð´Ð° Ñ… = "";';
      const tokens = tokenize(source);

      const stringToken = tokens.find(t => t.type === TokenType.STRING);
      expect(stringToken?.value).toBe('');
    });

    test('should handle string with escape sequences', () => {
      const source = 'Ñ‚Ð°Ò“Ð¹Ð¸Ñ€Ñ‘Ð±Ð°Ð½Ð´Ð° Ñ… = "ÑÐ°Ñ‚Ñ€\\nÐ½Ð°Ð² ÑÐ°Ñ‚Ñ€\\t\\r\\"";';
      const tokens = tokenize(source);

      const stringToken = tokens.find(t => t.type === TokenType.STRING);
      expect(stringToken?.value).toBe('ÑÐ°Ñ‚Ñ€\nÐ½Ð°Ð² ÑÐ°Ñ‚Ñ€\t\r"');
    });

    test('should handle string with Unicode characters', () => {
      const source = 'Ñ‚Ð°Ò“Ð¹Ð¸Ñ€Ñ‘Ð±Ð°Ð½Ð´Ð° Ñ… = "Ñ‚ÐµÑÑ‚ â˜€ ðŸŒ™ â­";';
      const tokens = tokenize(source);

      const stringToken = tokens.find(t => t.type === TokenType.STRING);
      expect(stringToken?.value).toBe('Ñ‚ÐµÑÑ‚ â˜€ ðŸŒ™ â­');
    });

    test('should handle unterminated string', () => {
      const source = 'Ñ‚Ð°Ò“Ð¹Ð¸Ñ€Ñ‘Ð±Ð°Ð½Ð´Ð° Ñ… = "unterminated';

      expect(() => tokenize(source)).toThrow();
    });

    test('should handle string with only quotes', () => {
      const source = 'Ñ‚Ð°Ò“Ð¹Ð¸Ñ€Ñ‘Ð±Ð°Ð½Ð´Ð° Ñ… = "\\"\\"\\"";';
      const tokens = tokenize(source);

      const stringToken = tokens.find(t => t.type === TokenType.STRING);
      expect(stringToken?.value).toBe('"""');
    });
  });

  describe('Number literal edge cases', () => {
    test('should handle integer zero', () => {
      const source = '0';
      const tokens = tokenize(source);

      expect(tokens[0].type).toBe(TokenType.NUMBER);
      expect(tokens[0].value).toBe('0');
    });

    test('should handle decimal with leading zero', () => {
      const source = '0.123';
      const tokens = tokenize(source);

      expect(tokens[0].type).toBe(TokenType.NUMBER);
      expect(tokens[0].value).toBe('0.123');
    });

    test('should handle large numbers', () => {
      const source = '123456789.987654321';
      const tokens = tokenize(source);

      expect(tokens[0].type).toBe(TokenType.NUMBER);
      expect(tokens[0].value).toBe('123456789.987654321');
    });

    test('should handle scientific notation if supported', () => {
      const source = '1e10 2.5e-3';
      const tokens = tokenize(source);

      // This might tokenize as separate tokens depending on implementation
      expect(tokens[0].type).toBe(TokenType.NUMBER);
    });

    test('should handle decimal without leading digit', () => {
      const source = '.123';
      const tokens = tokenize(source);

      // Should tokenize as . followed by 123 or as a single number
      expect(tokens[0].type).toBe(TokenType.DOT);
      expect(tokens[1].type).toBe(TokenType.NUMBER);
      expect(tokens[1].value).toBe('123');
    });

    test('should handle multiple decimal points as separate tokens', () => {
      // Test case should expect an error for invalid syntax
      const source = '1.2.3';

      expect(() => tokenize(source)).toThrow('multiple decimal points');
    });
  });

  describe('Operator combinations', () => {
    test('should handle increment and decrement operators', () => {
      const source = '++ -- += -= *= /= %= **= <<= >>= &= |= ^=';
      const tokens = tokenize(source);

      const expectedTypes = [
        TokenType.INCREMENT,
        TokenType.DECREMENT,
        TokenType.PLUS_ASSIGN,
        TokenType.MINUS_ASSIGN,
        TokenType.MULTIPLY_ASSIGN,
        TokenType.DIVIDE_ASSIGN,
        TokenType.MODULO_ASSIGN,
        TokenType.EXPONENT_ASSIGN,
        TokenType.LEFT_SHIFT_ASSIGN,
        TokenType.RIGHT_SHIFT_ASSIGN,
        TokenType.BITWISE_AND_ASSIGN,
        TokenType.BITWISE_OR_ASSIGN,
        TokenType.BITWISE_XOR_ASSIGN,
      ];

      expectedTypes.forEach((expectedType, index) => {
        expect(tokens[index].type).toBe(expectedType);
      });
    });

    test('should handle bitwise operators', () => {
      const source = '& | ^ ~ << >> >>> &= |= ^= <<= >>= >>>=';
      const tokens = tokenize(source);

      const bitwiseTokens = tokens.filter(t => t.type !== TokenType.EOF);
      expect(bitwiseTokens.length).toBeGreaterThan(0);

      // Check for bitwise AND
      expect(bitwiseTokens.some(t => t.type === TokenType.BITWISE_AND)).toBe(true);
      // Check for bitwise OR
      expect(bitwiseTokens.some(t => t.type === TokenType.BITWISE_OR)).toBe(true);
    });

    test('should handle exponentiation operator', () => {
      const source = '2 ** 3 **= 4';
      const tokens = tokenize(source);

      expect(tokens[1].type).toBe(TokenType.EXPONENT);
      expect(tokens[3].type).toBe(TokenType.EXPONENT_ASSIGN);
    });

    test('should handle ternary operator components', () => {
      const source = 'Ð° ? Ð± : Ð²';
      const tokens = tokenize(source);

      expect(tokens[1].type).toBe(TokenType.QUESTION);
      expect(tokens[3].type).toBe(TokenType.COLON);
    });

    test('should handle null coalescing if supported', () => {
      const source = 'Ð° ?? Ð±';
      const tokens = tokenize(source);

      // Might be tokenized as two separate ? tokens or as NULL_COALESCING
      const questionTokens = tokens.filter(t => t.type === TokenType.QUESTION);
      expect(questionTokens.length).toBeGreaterThanOrEqual(1);
    });
  });

  describe('Identifier edge cases', () => {
    test('should handle identifiers with numbers', () => {
      const source = 'Ð¿ÐµÑ€ÐµÐ¼ÐµÐ½Ð½Ð°Ñ1 Ñ„ÑƒÐ½ÐºÑ†Ð¸Ñ2Ñ‚ÐµÑÑ‚ Ð¿ÐµÑ€ÐµÐ¼ÐµÐ½Ð½Ð°Ñ_123';
      const tokens = tokenize(source);

      expect(tokens[0].type).toBe(TokenType.IDENTIFIER);
      expect(tokens[0].value).toBe('Ð¿ÐµÑ€ÐµÐ¼ÐµÐ½Ð½Ð°Ñ1');
      expect(tokens[1].type).toBe(TokenType.IDENTIFIER);
      expect(tokens[1].value).toBe('Ñ„ÑƒÐ½ÐºÑ†Ð¸Ñ2Ñ‚ÐµÑÑ‚');
      expect(tokens[2].type).toBe(TokenType.IDENTIFIER);
      expect(tokens[2].value).toBe('Ð¿ÐµÑ€ÐµÐ¼ÐµÐ½Ð½Ð°Ñ_123');
    });

    test('should handle identifiers with underscores', () => {
      const source = '_Ð¿Ñ€Ð¸Ð²Ð°Ñ‚Ð½Ð°Ñ __Ð²Ð½ÑƒÑ‚Ñ€ÐµÐ½Ð½ÑÑ _';
      const tokens = tokenize(source);

      expect(tokens[0].type).toBe(TokenType.IDENTIFIER);
      expect(tokens[0].value).toBe('_Ð¿Ñ€Ð¸Ð²Ð°Ñ‚Ð½Ð°Ñ');
      expect(tokens[1].type).toBe(TokenType.IDENTIFIER);
      expect(tokens[1].value).toBe('__Ð²Ð½ÑƒÑ‚Ñ€ÐµÐ½Ð½ÑÑ');
      expect(tokens[2].type).toBe(TokenType.IDENTIFIER);
      expect(tokens[2].value).toBe('_');
    });

    test('should handle very long identifiers', () => {
      const source =
        'Ð¾Ñ‡ÐµÐ½ÑŒ_Ð´Ð»Ð¸Ð½Ð½Ð¾Ðµ_Ð¸Ð¼Ñ_Ð¿ÐµÑ€ÐµÐ¼ÐµÐ½Ð½Ð¾Ð¹_ÐºÐ¾Ñ‚Ð¾Ñ€Ð¾Ðµ_ÑÐ¾Ð´ÐµÑ€Ð¶Ð¸Ñ‚_Ð¼Ð½Ð¾Ð³Ð¾_ÑÐ¸Ð¼Ð²Ð¾Ð»Ð¾Ð²_Ð¸_Ð¼Ð¾Ð¶ÐµÑ‚_Ð²Ñ‹Ð·Ð²Ð°Ñ‚ÑŒ_Ð¿Ñ€Ð¾Ð±Ð»ÐµÐ¼Ñ‹';
      const tokens = tokenize(source);

      expect(tokens[0].type).toBe(TokenType.IDENTIFIER);
      expect(tokens[0].value).toBe(source);
    });
  });

  describe('Whitespace and newline handling', () => {
    test('should handle mixed whitespace', () => {
      const source = 'Ñ‚Ð°Ò“Ð¹Ð¸Ñ€Ñ‘Ð±Ð°Ð½Ð´Ð°\t  \r\n  Ð½Ð¾Ð¼ = 5;';
      const tokens = tokenize(source);

      const nonWhitespaceTokens = tokens.filter(
        t => t.type !== TokenType.WHITESPACE && t.type !== TokenType.NEWLINE
      );
      expect(nonWhitespaceTokens).toHaveLength(6); // Ñ‚Ð°Ò“Ð¹Ð¸Ñ€Ñ‘Ð±Ð°Ð½Ð´Ð°, Ð½Ð¾Ð¼, =, 5, ;, EOF
    });

    test('should handle Windows line endings', () => {
      const source = 'Ñ‚Ð°Ò“Ð¹Ð¸Ñ€Ñ‘Ð±Ð°Ð½Ð´Ð° Ñ… = 5;\r\nÑ„ÑƒÐ½ÐºÑÐ¸Ñ Ñ‚ÐµÑÑ‚() {}';
      const tokens = tokenize(source);

      const keywordTokens = tokens.filter(
        t => t.type === TokenType.Ð¢ÐÒ’Ð™Ð˜Ð ÐÐ‘ÐÐÐ”Ð || t.type === TokenType.Ð¤Ð£ÐÐšÐ¡Ð˜Ð¯
      );
      expect(keywordTokens).toHaveLength(2);
    });

    test('should handle Mac line endings', () => {
      const source = 'Ñ‚Ð°Ò“Ð¹Ð¸Ñ€Ñ‘Ð±Ð°Ð½Ð´Ð° Ñ… = 5;\rÑ„ÑƒÐ½ÐºÑÐ¸Ñ Ñ‚ÐµÑÑ‚() {}';
      const tokens = tokenize(source);

      const keywordTokens = tokens.filter(
        t => t.type === TokenType.Ð¢ÐÒ’Ð™Ð˜Ð ÐÐ‘ÐÐÐ”Ð || t.type === TokenType.Ð¤Ð£ÐÐšÐ¡Ð˜Ð¯
      );
      expect(keywordTokens).toHaveLength(2);
    });

    test('should handle empty lines', () => {
      const source = 'Ñ‚Ð°Ò“Ð¹Ð¸Ñ€Ñ‘Ð±Ð°Ð½Ð´Ð° Ñ… = 5;\n\n\nÑ„ÑƒÐ½ÐºÑÐ¸Ñ Ñ‚ÐµÑÑ‚() {}';
      const tokens = tokenize(source);

      const keywordTokens = tokens.filter(
        t => t.type === TokenType.Ð¢ÐÒ’Ð™Ð˜Ð ÐÐ‘ÐÐÐ”Ð || t.type === TokenType.Ð¤Ð£ÐÐšÐ¡Ð˜Ð¯
      );
      expect(keywordTokens).toHaveLength(2);
    });
  });

  describe('Position tracking', () => {
    test('should track line and column correctly with tabs', () => {
      const source = 'Ñ‚Ð°Ò“Ð¹Ð¸Ñ€Ñ‘Ð±Ð°Ð½Ð´Ð°\tÐ½Ð¾Ð¼\n\tÐ·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ðµ';
      const tokens = tokenize(source);

      expect(tokens[0].line).toBe(1);
      expect(tokens[0].column).toBe(1);

      // Find the identifier on the second line
      const secondLineToken = tokens.find(t => t.line === 2 && t.type === TokenType.IDENTIFIER);
      expect(secondLineToken?.value).toBe('Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ðµ');
    });

    test('should handle position tracking with Unicode characters', () => {
      const source = 'Ñ‚Ð°Ò“Ð¹Ð¸Ñ€Ñ‘Ð±Ð°Ð½Ð´Ð° Ð½Ð¾Ð¼ = "Ñ‚ÐµÑÑ‚ðŸŒŸ";';
      const tokens = tokenize(source);

      // Should still track positions correctly despite Unicode
      expect(tokens[0].line).toBe(1);
      expect(tokens[0].column).toBe(1);

      // Check that the emoji is preserved in the string
      const stringToken = tokens.find(t => t.type === TokenType.STRING);
      expect(stringToken?.value).toContain('ðŸŒŸ');
    });
  });

  describe('Keyword variants', () => {
    test('should recognize alternative keyword spellings', () => {
      const source = 'Ñ„ÑƒÐ½ÐºÑÐ¸Ñ Ñ„ÑƒÐ½ÐºÑ†Ð¸Ñ'; // Only test the known spelling
      const tokens = tokenize(source);

      expect(tokens[0].type).toBe(TokenType.Ð¤Ð£ÐÐšÐ¡Ð˜Ð¯);
      expect(tokens[1].type).toBe(TokenType.Ð¤Ð£ÐÐšÐ¡Ð˜Ð¯);
    });

    test('should handle all boolean literals', () => {
      const source = 'Ð´ÑƒÑ€ÑƒÑÑ‚ Ð½Ð¾Ð´ÑƒÑ€ÑƒÑÑ‚ Ñ…Ð¾Ð»Ó£ Ð±ÐµÒ›Ð¸Ð¼Ð°Ñ‚';
      const tokens = tokenize(source);

      expect(tokens[0].type).toBe(TokenType.Ð”Ð£Ð Ð£Ð¡Ð¢);
      expect(tokens[1].type).toBe(TokenType.ÐÐžÐ”Ð£Ð Ð£Ð¡Ð¢);
      expect(tokens[2].type).toBe(TokenType.Ð¥ÐžÐ›Ó¢);
      expect(tokens[3].type).toBe(TokenType.Ð‘Ð•ÒšÐ˜ÐœÐÐ¢);
    });

    test('should handle all type keywords', () => {
      const source = 'ÑÐ°Ñ‚Ñ€ Ñ€Ð°Ò›Ð°Ð¼ Ð¼Ð°Ð½Ñ‚Ð¸Ò›Ó£';
      const tokens = tokenize(source);

      expect(tokens[0].type).toBe(TokenType.Ð¡ÐÐ¢Ð );
      expect(tokens[1].type).toBe(TokenType.Ð ÐÒšÐÐœ);
      expect(tokens[2].type).toBe(TokenType.ÐœÐÐÐ¢Ð˜ÒšÓ¢);
    });
  });

  describe('Complex operator sequences', () => {
    test('should handle complex assignment operators', () => {
      const source = 'Ð° += Ð± -= Ð² *= Ð³ /= Ð´ %= Ðµ **= Ð¶';
      const tokens = tokenize(source);

      const assignmentTokens = tokens.filter(
        t =>
          t.type === TokenType.PLUS_ASSIGN ||
          t.type === TokenType.MINUS_ASSIGN ||
          t.type === TokenType.MULTIPLY_ASSIGN ||
          t.type === TokenType.DIVIDE_ASSIGN ||
          t.type === TokenType.MODULO_ASSIGN ||
          t.type === TokenType.EXPONENT_ASSIGN
      );
      expect(assignmentTokens).toHaveLength(6);
    });

    test('should handle operator precedence tokens', () => {
      const source = '(Ð° + Ð±) * (Ð² - Ð³) / (Ð´ % Ðµ)';
      const tokens = tokenize(source);

      const parenTokens = tokens.filter(
        t => t.type === TokenType.LEFT_PAREN || t.type === TokenType.RIGHT_PAREN
      );
      expect(parenTokens).toHaveLength(6);
    });
  });

  describe('Method and property access', () => {
    test('should handle object property access', () => {
      const source = 'Ð¾Ð±ÑŠÐµÐºÑ‚.ÑÐ²Ð¾Ð¹ÑÑ‚Ð²Ð¾[Ð¸Ð½Ð´ÐµÐºÑ].Ð¼ÐµÑ‚Ð¾Ð´()';
      const tokens = tokenize(source);

      const dotTokens = tokens.filter(t => t.type === TokenType.DOT);
      expect(dotTokens).toHaveLength(2);

      const bracketTokens = tokens.filter(
        t => t.type === TokenType.LEFT_BRACKET || t.type === TokenType.RIGHT_BRACKET
      );
      expect(bracketTokens).toHaveLength(2);
    });

    test('should handle method chaining', () => {
      const source = 'Ð¾Ð±ÑŠÐµÐºÑ‚.Ð¼ÐµÑ‚Ð¾Ð´1().Ð¼ÐµÑ‚Ð¾Ð´2().Ð¼ÐµÑ‚Ð¾Ð´3()';
      const tokens = tokenize(source);

      const dotTokens = tokens.filter(t => t.type === TokenType.DOT);
      expect(dotTokens).toHaveLength(3);
    });
  });

  describe('Array and object literals', () => {
    test('should handle complex array literals', () => {
      const source = '[1, "ÑÑ‚Ñ€Ð¾ÐºÐ°", Ð´ÑƒÑ€ÑƒÑÑ‚, [Ð²Ð»Ð¾Ð¶ÐµÐ½Ð½Ñ‹Ð¹, Ð¼Ð°ÑÑÐ¸Ð²], {Ð¾Ð±ÑŠÐµÐºÑ‚: Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ðµ}]';
      const tokens = tokenize(source);

      const bracketTokens = tokens.filter(
        t => t.type === TokenType.LEFT_BRACKET || t.type === TokenType.RIGHT_BRACKET
      );
      expect(bracketTokens).toHaveLength(4);

      const braceTokens = tokens.filter(
        t => t.type === TokenType.LEFT_BRACE || t.type === TokenType.RIGHT_BRACE
      );
      expect(braceTokens).toHaveLength(2);
    });

    test('should handle object destructuring syntax', () => {
      const source = 'Ñ‚Ð°Ò“Ð¹Ð¸Ñ€Ñ‘Ð±Ð°Ð½Ð´Ð° {Ð°, Ð±: Ð²} = Ð¾Ð±ÑŠÐµÐºÑ‚;';
      const tokens = tokenize(source);

      const braceTokens = tokens.filter(
        t => t.type === TokenType.LEFT_BRACE || t.type === TokenType.RIGHT_BRACE
      );
      expect(braceTokens).toHaveLength(2);
    });
  });

  describe('Invalid character handling', () => {
    test('should handle unexpected characters gracefully', () => {
      const source = 'Ñ‚Ð°Ò“Ð¹Ð¸Ñ€Ñ‘Ð±Ð°Ð½Ð´Ð° @ Ð½Ð¾Ð¼ = #Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ðµ;';

      expect(() => tokenize(source)).toThrow();
    });

    test('should handle control characters', () => {
      const source = 'Ñ‚Ð°Ò“Ð¹Ð¸Ñ€Ñ‘Ð±Ð°Ð½Ð´Ð°\u0001Ð½Ð¾Ð¼ = Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ðµ;';

      expect(() => tokenize(source)).toThrow();
    });
  });

  describe('End of file handling', () => {
    test('should handle empty file', () => {
      const source = '';
      const tokens = tokenize(source);

      expect(tokens).toHaveLength(1);
      expect(tokens[0].type).toBe(TokenType.EOF);
    });

    test('should handle file with only whitespace', () => {
      const source = '   \t\n\r\n  ';
      const tokens = tokenize(source);

      expect(tokens[tokens.length - 1].type).toBe(TokenType.EOF);
    });

    test('should handle file with only comments', () => {
      const source = '// Ñ‚Ð¾Ð»ÑŒÐºÐ¾ ÐºÐ¾Ð¼Ð¼ÐµÐ½Ñ‚Ð°Ñ€Ð¸Ð¹\n// ÐµÑ‰Ðµ Ð¾Ð´Ð¸Ð½ ÐºÐ¾Ð¼Ð¼ÐµÐ½Ñ‚Ð°Ñ€Ð¸Ð¹';
      const tokens = tokenize(source);

      expect(tokens[tokens.length - 1].type).toBe(TokenType.EOF);
    });
  });

  describe('String method tokens', () => {
    test('should tokenize string method identifiers', () => {
      const source = 'ÑÐ°Ñ‚Ñ€_Ð¼ÐµÑ‚Ð¾Ð´Ò³Ð¾ Ð´Ð°Ñ€Ð¾Ð·Ð¸Ð¸_ÑÐ°Ñ‚Ñ€ Ð¿Ð°Ð¹Ð²Ð°ÑÑ‚Ð°Ð½ Ò·Ð¾Ð¹Ð¸Ð²Ð°Ð·ÐºÑƒÐ½Ó£ Ò·ÑƒÐ´Ð¾ÐºÑƒÐ½Ó£';
      const tokens = tokenize(source);

      expect(tokens[0].type).toBe(TokenType.Ð¡ÐÐ¢Ð _ÐœÐ•Ð¢ÐžÐ”Ò²Ðž);
      expect(tokens[1].type).toBe(TokenType.Ð”ÐÐ ÐžÐ—Ð˜Ð˜_Ð¡ÐÐ¢Ð );
      expect(tokens[2].type).toBe(TokenType.ÐŸÐÐ™Ð’ÐÐ¡Ð¢ÐÐ);
      expect(tokens[3].type).toBe(TokenType.Ò¶ÐžÐ™Ð˜Ð’ÐÐ—ÐšÐ£ÐÓ¢);
      expect(tokens[4].type).toBe(TokenType.Ò¶Ð£Ð”ÐžÐšÐ£ÐÓ¢);
    });

    test('should tokenize array method identifiers', () => {
      const source = 'Ñ€Ó¯Ð¹Ñ…Ð°Ñ‚ Ð¸Ð»Ð¾Ð²Ð° Ð±Ð°Ñ€Ð¾Ð²Ð°Ñ€Ð´Ð°Ð½ Ð´Ð°Ñ€Ð¾Ð·Ó£ Ñ…Ð°Ñ€Ð¸Ñ‚Ð° Ñ„Ð¸Ð»Ñ‚Ñ€ ÐºÐ¾Ñ„Ñ‚Ð°Ð½';
      const tokens = tokenize(source);

      expect(tokens[0].type).toBe(TokenType.Ð Ó®Ð™Ð¥ÐÐ¢);
      expect(tokens[1].type).toBe(TokenType.Ð˜Ð›ÐžÐ’Ð);
      expect(tokens[2].type).toBe(TokenType.Ð‘ÐÐ ÐžÐ’ÐÐ Ð”ÐÐ);
      expect(tokens[3].type).toBe(TokenType.Ð”ÐÐ ÐžÐ—Ó¢);
      expect(tokens[4].type).toBe(TokenType.Ð¥ÐÐ Ð˜Ð¢Ð);
      expect(tokens[5].type).toBe(TokenType.Ð¤Ð˜Ð›Ð¢Ð );
      expect(tokens[6].type).toBe(TokenType.ÐšÐžÐ¤Ð¢ÐÐ);
    });

    test('should tokenize object method identifiers', () => {
      const source = 'Ð¾Ð±ÑŠÐµÐºÑ‚ ÐºÐ°Ð»Ð¸Ð´Ò³Ð¾ Ò›Ð¸Ð¼Ð°Ñ‚Ò³Ð¾';
      const tokens = tokenize(source);

      expect(tokens[0].type).toBe(TokenType.ÐžÐ‘ÐªÐ•ÐšÐ¢);
      expect(tokens[1].type).toBe(TokenType.ÐšÐÐ›Ð˜Ð”Ò²Ðž);
      expect(tokens[2].type).toBe(TokenType.ÒšÐ˜ÐœÐÐ¢Ò²Ðž);
    });
  });

  describe('Advanced type system tokens', () => {
    test('should tokenize advanced type keywords', () => {
      const source = 'Ñ‚Ð°Ð½Ò³Ð¾Ñ…Ð¾Ð½Ó£ Ð±ÐµÐ½Ð°Ð·Ð¸Ñ€ ÐºÐ°Ð»Ð¸Ð´Ò³Ð¾Ð¸ Ð¸Ð½Ñ„ÐµÑ€ Ð½Ð¾Ð¼Ñ„Ð°Ð·Ð¾';
      const tokens = tokenize(source);

      expect(tokens[0].type).toBe(TokenType.Ð¢ÐÐÒ²ÐžÐ¥ÐžÐÓ¢);
      expect(tokens[1].type).toBe(TokenType.Ð‘Ð•ÐÐÐ—Ð˜Ð );
      expect(tokens[2].type).toBe(TokenType.ÐšÐÐ›Ð˜Ð”Ò²ÐžÐ˜);
      expect(tokens[3].type).toBe(TokenType.Ð˜ÐÐ¤Ð•Ð );
      expect(tokens[4].type).toBe(TokenType.ÐÐžÐœÐ¤ÐÐ—Ðž);
    });

    test('should tokenize class-related keywords', () => {
      const source =
        'ÑÐ¸Ð½Ñ„ Ð¼ÐµÑ€Ð¾Ñ Ñ‚Ð°Ñ‚Ð±Ð¸Ò› ÑÑƒÐ¿ÐµÑ€ ÐºÐ¾Ð½ÑÑ‚Ñ€ÑƒÐºÑ‚Ð¾Ñ€ Ñ…Ð¾ÑÑƒÑÓ£ Ð¼ÑƒÒ³Ð¾Ñ„Ð¸Ð·Ð°Ñ‚ÑˆÑƒÐ´Ð° Ò·Ð°Ð¼ÑŠÐ¸ÑÑ‚Ó£ ÑÑ‚Ð°Ñ‚Ð¸ÐºÓ£ Ð¼Ð°Ð²Ò³ÑƒÐ¼';
      const tokens = tokenize(source);

      expect(tokens[0].type).toBe(TokenType.Ð¡Ð˜ÐÐ¤);
      expect(tokens[1].type).toBe(TokenType.ÐœÐ•Ð ÐžÐ¡);
      expect(tokens[2].type).toBe(TokenType.Ð¢ÐÐ¢Ð‘Ð˜Òš);
      expect(tokens[3].type).toBe(TokenType.Ð¡Ð£ÐŸÐ•Ð );
      expect(tokens[4].type).toBe(TokenType.ÐšÐžÐÐ¡Ð¢Ð Ð£ÐšÐ¢ÐžÐ );
      expect(tokens[5].type).toBe(TokenType.Ð¥ÐžÐ¡Ð£Ð¡Ó¢);
      expect(tokens[6].type).toBe(TokenType.ÐœÐ£Ò²ÐžÐ¤Ð˜Ð—ÐÐ¢Ð¨Ð£Ð”Ð);
      expect(tokens[7].type).toBe(TokenType.Ò¶ÐÐœÐªÐ˜Ð¯Ð¢Ó¢);
      expect(tokens[8].type).toBe(TokenType.Ð¡Ð¢ÐÐ¢Ð˜ÐšÓ¢);
      expect(tokens[9].type).toBe(TokenType.ÐœÐÐ’Ò²Ð£Ðœ);
    });

    test('should tokenize async keywords', () => {
      const source = 'Ò³Ð°Ð¼Ð·Ð°Ð¼Ð¾Ð½ Ð¸Ð½Ñ‚Ð¸Ð·Ð¾Ñ€ Ð²Ð°ÑŠÐ´Ð°';
      const tokens = tokenize(source);

      expect(tokens[0].type).toBe(TokenType.Ò²ÐÐœÐ—ÐÐœÐžÐ);
      expect(tokens[1].type).toBe(TokenType.Ð˜ÐÐ¢Ð˜Ð—ÐžÐ );
      expect(tokens[2].type).toBe(TokenType.Ð’ÐÐªÐ”Ð);
    });
  });

  describe('Complex scenarios', () => {
    test('should handle complex expressions with mixed operators', () => {
      const source = '(Ð° += Ð±++) !== Ð² && Ð³ || Ð´ ? Ðµ : Ð¶ ** Ð·';
      const tokens = tokenize(source);

      // Should tokenize all operators correctly
      const operatorTokens = tokens.filter(t =>
        [
          TokenType.PLUS_ASSIGN,
          TokenType.INCREMENT,
          TokenType.NOT_EQUAL,
          TokenType.AND,
          TokenType.OR,
          TokenType.QUESTION,
          TokenType.COLON,
          TokenType.EXPONENT,
        ].includes(t.type)
      );
      expect(operatorTokens.length).toBeGreaterThanOrEqual(7);
    });

    test('should handle template literal-like constructs', () => {
      // Template literals are not supported, so expect an error
      const source = '`template ${variable} literal`';

      expect(() => tokenize(source)).toThrow('Unexpected character');
    });

    test('should handle regex-like constructs', () => {
      const source = '/pattern/flags';
      const tokens = tokenize(source);

      // Should tokenize as divide operators and identifiers
      expect(tokens[0].type).toBe(TokenType.DIVIDE);
      expect(tokens[2].type).toBe(TokenType.DIVIDE);
    });
  });
});
